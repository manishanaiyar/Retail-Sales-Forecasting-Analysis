{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "63b01936-b0f5-455c-bca6-60da4a0a1213",
   "metadata": {},
   "source": [
    "## 1.Load train_Data from CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "efb8aa09-2d78-48b9-b8f7-bc25edf96cbb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Row ID        Order ID  Order Date   Ship Date       Ship Mode Customer ID  \\\n",
      "0       1  CA-2017-152156  08/11/2017  11/11/2017    Second Class    CG-12520   \n",
      "1       2  CA-2017-152156  08/11/2017  11/11/2017    Second Class    CG-12520   \n",
      "2       3  CA-2017-138688  12/06/2017  16/06/2017    Second Class    DV-13045   \n",
      "3       4  US-2016-108966  11/10/2016  18/10/2016  Standard Class    SO-20335   \n",
      "4       5  US-2016-108966  11/10/2016  18/10/2016  Standard Class    SO-20335   \n",
      "\n",
      "     Customer Name    Segment        Country             City       State  \\\n",
      "0      Claire Gute   Consumer  United States        Henderson    Kentucky   \n",
      "1      Claire Gute   Consumer  United States        Henderson    Kentucky   \n",
      "2  Darrin Van Huff  Corporate  United States      Los Angeles  California   \n",
      "3   Sean O'Donnell   Consumer  United States  Fort Lauderdale     Florida   \n",
      "4   Sean O'Donnell   Consumer  United States  Fort Lauderdale     Florida   \n",
      "\n",
      "   Postal Code Region       Product ID         Category Sub-Category  \\\n",
      "0      42420.0  South  FUR-BO-10001798        Furniture    Bookcases   \n",
      "1      42420.0  South  FUR-CH-10000454        Furniture       Chairs   \n",
      "2      90036.0   West  OFF-LA-10000240  Office Supplies       Labels   \n",
      "3      33311.0  South  FUR-TA-10000577        Furniture       Tables   \n",
      "4      33311.0  South  OFF-ST-10000760  Office Supplies      Storage   \n",
      "\n",
      "                                        Product Name     Sales  \n",
      "0                  Bush Somerset Collection Bookcase  261.9600  \n",
      "1  Hon Deluxe Fabric Upholstered Stacking Chairs,...  731.9400  \n",
      "2  Self-Adhesive Address Labels for Typewriters b...   14.6200  \n",
      "3      Bretford CR4500 Series Slim Rectangular Table  957.5775  \n",
      "4                     Eldon Fold 'N Roll Cart System   22.3680  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def load_data(path):\n",
    "    \"\"\"Loads data from a CSV file.\"\"\"\n",
    "    try:\n",
    "        df = pd.read_csv(path)\n",
    "        return df\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: File not found at {path}\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        return None\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    data_path = 'train.csv'\n",
    "    df = load_data(data_path)\n",
    "    if df is not None:\n",
    "        print(df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e96500c-efd4-4faa-bf69-d76ec74ce1f5",
   "metadata": {},
   "source": [
    "## 2. Fetch US Holiday Data from API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "661f8de9-6bad-4815-9de3-ee6d7b0d228b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>holiday_name</th>\n",
       "      <th>holiday_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2015-01-01</td>\n",
       "      <td>New Year's Day</td>\n",
       "      <td>National holiday</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2015-01-19</td>\n",
       "      <td>Martin Luther King Jr. Day</td>\n",
       "      <td>National holiday</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2015-02-16</td>\n",
       "      <td>Presidents' Day</td>\n",
       "      <td>National holiday</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2015-05-25</td>\n",
       "      <td>Memorial Day</td>\n",
       "      <td>National holiday</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2015-07-03</td>\n",
       "      <td>Independence Day (substitute)</td>\n",
       "      <td>National holiday</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        date                   holiday_name      holiday_type\n",
       "0 2015-01-01                 New Year's Day  National holiday\n",
       "1 2015-01-19     Martin Luther King Jr. Day  National holiday\n",
       "2 2015-02-16                Presidents' Day  National holiday\n",
       "3 2015-05-25                   Memorial Day  National holiday\n",
       "4 2015-07-03  Independence Day (substitute)  National holiday"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "api_key = \"URbJpoUsc2yXoYcezjO3ZttvcNEMWqrs\"\n",
    "\n",
    "def get_us_holidays(year):\n",
    "    url = f\"https://calendarific.com/api/v2/holidays?&api_key={api_key}&country=US&year={year}&type=national\"\n",
    "    response = requests.get(url)\n",
    "    data = response.json()\n",
    "    \n",
    "    holidays = [\n",
    "        {\n",
    "            \"date\": item[\"date\"][\"iso\"],\n",
    "            \"holiday_name\": item[\"name\"],\n",
    "            \"holiday_type\": \", \".join(item[\"type\"])\n",
    "        }\n",
    "        for item in data[\"response\"][\"holidays\"]\n",
    "    ]\n",
    "    return pd.DataFrame(holidays)\n",
    "\n",
    "# Fetch holidays from 2015 to 2018 (you can change the range)\n",
    "all_years = []\n",
    "for year in range(2015, 2019):\n",
    "    df = get_us_holidays(year)\n",
    "    all_years.append(df)\n",
    "\n",
    "# Combine all holiday data\n",
    "holidays_df = pd.concat(all_years, ignore_index=True)\n",
    "holidays_df['date'] = pd.to_datetime(holidays_df['date'])\n",
    "holidays_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "bd2feffa-0c2a-4232-9ad3-a1dcb83a2868",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to CSV\n",
    "holidays_df.to_csv('us_holidays_2015_2018.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7634085c-6cfa-469c-9466-1ffc6cfdc14a",
   "metadata": {},
   "source": [
    "## 3. Extract CPI Data from Website"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "ea952c18-e21b-4bf7-990f-47e37da83fe1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1 tables\n",
      "\n",
      "Table 0\n",
      "   Year      Jan      Feb      Mar      Apr      May      Jun      Jul  \\\n",
      "0  2025  317.671  319.082  319.799      NaN      NaN      NaN      NaN   \n",
      "1  2024  308.417  310.326  312.332  313.548  314.069  314.175  314.540   \n",
      "2  2023  299.170  300.840  301.836  303.363  304.127  305.109  305.691   \n",
      "3  2022  281.148  283.716  287.504  289.109  292.296  296.311  296.276   \n",
      "4  2021  261.582  263.014  264.877  267.054  269.195  271.696  273.003   \n",
      "\n",
      "       Aug      Sep      Oct      Nov      Dec     Ave.  \n",
      "0      NaN      NaN      NaN      NaN      NaN      NaN  \n",
      "1  314.796  315.301  315.664  315.493  315.605  313.689  \n",
      "2  307.026  307.789  307.671  307.051  306.746  304.702  \n",
      "3  296.171  296.808  298.012  297.711  296.797  292.655  \n",
      "4  273.567  274.310  276.589  277.948  278.802  270.970  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# URL with historical CPI table (InflationData.com)\n",
    "url = \"https://www.inflationdata.com/Inflation/Consumer_Price_Index/HistoricalCPI.aspx\"\n",
    "\n",
    "# Read all tables\n",
    "tables = pd.read_html(url)\n",
    "\n",
    "# Show how many tables were found\n",
    "print(f\"Found {len(tables)} tables\")\n",
    "\n",
    "# Preview all tables to find the right one\n",
    "for i, table in enumerate(tables):\n",
    "    print(f\"\\nTable {i}\")\n",
    "    print(table.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f36bbd7a-05e5-4ee0-95a5-7fbdc159e260",
   "metadata": {},
   "source": [
    "##  Clean and Save CPI Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "3ea30b51-3a5b-4943-9c9e-d913e5dfc094",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Cleaned CPI data saved as 'cleaned_cpi_data.csv'\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Step 1: Read CPI table\n",
    "url = \"https://www.inflationdata.com/Inflation/Consumer_Price_Index/HistoricalCPI.aspx\"\n",
    "tables = pd.read_html(url)\n",
    "cpi_table = tables[0]\n",
    "\n",
    "# Step 2: Remove the \"Ave.\" column if it exists\n",
    "if \"Ave.\" in cpi_table.columns:\n",
    "    cpi_table = cpi_table.drop(columns=[\"Ave.\"])\n",
    "\n",
    "# Step 3: Melt the table (Year-Month-CPI)\n",
    "cpi_long = cpi_table.melt(id_vars=[\"Year\"], var_name=\"Month\", value_name=\"CPI\")\n",
    "\n",
    "# Step 4: Drop missing CPI rows\n",
    "cpi_cleaned = cpi_long.dropna(subset=[\"CPI\"]).copy()\n",
    "\n",
    "# Step 5: Clean up and convert types\n",
    "cpi_cleaned[\"Year\"] = cpi_cleaned[\"Year\"].astype(int)\n",
    "cpi_cleaned[\"Month\"] = cpi_cleaned[\"Month\"].str.strip()\n",
    "cpi_cleaned[\"CPI\"] = pd.to_numeric(cpi_cleaned[\"CPI\"], errors=\"coerce\")\n",
    "\n",
    "# Step 6: Create a datetime column\n",
    "cpi_cleaned[\"Date\"] = pd.to_datetime(cpi_cleaned[\"Month\"] + \" \" + cpi_cleaned[\"Year\"].astype(str), format=\"%b %Y\")\n",
    "\n",
    "# Step 7: Save cleaned data\n",
    "cpi_cleaned[[\"Date\", \"CPI\"]].to_csv(\"cleaned_cpi_data.csv\", index=False)\n",
    "\n",
    "print(\"✅ Cleaned CPI data saved as 'cleaned_cpi_data.csv'\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd0a283f-243d-4b96-8aa2-ae31e40f8f81",
   "metadata": {},
   "source": [
    "##  Load and Inspect Training Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "eee7f7ab-ac6c-464f-b801-3d1a9d53eb3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 9800 entries, 0 to 9799\n",
      "Data columns (total 18 columns):\n",
      " #   Column         Non-Null Count  Dtype  \n",
      "---  ------         --------------  -----  \n",
      " 0   Row ID         9800 non-null   int64  \n",
      " 1   Order ID       9800 non-null   object \n",
      " 2   Order Date     9800 non-null   object \n",
      " 3   Ship Date      9800 non-null   object \n",
      " 4   Ship Mode      9800 non-null   object \n",
      " 5   Customer ID    9800 non-null   object \n",
      " 6   Customer Name  9800 non-null   object \n",
      " 7   Segment        9800 non-null   object \n",
      " 8   Country        9800 non-null   object \n",
      " 9   City           9800 non-null   object \n",
      " 10  State          9800 non-null   object \n",
      " 11  Postal Code    9789 non-null   float64\n",
      " 12  Region         9800 non-null   object \n",
      " 13  Product ID     9800 non-null   object \n",
      " 14  Category       9800 non-null   object \n",
      " 15  Sub-Category   9800 non-null   object \n",
      " 16  Product Name   9800 non-null   object \n",
      " 17  Sales          9800 non-null   float64\n",
      "dtypes: float64(2), int64(1), object(15)\n",
      "memory usage: 1.3+ MB\n",
      "None\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Row ID</th>\n",
       "      <th>Order ID</th>\n",
       "      <th>Order Date</th>\n",
       "      <th>Ship Date</th>\n",
       "      <th>Ship Mode</th>\n",
       "      <th>Customer ID</th>\n",
       "      <th>Customer Name</th>\n",
       "      <th>Segment</th>\n",
       "      <th>Country</th>\n",
       "      <th>City</th>\n",
       "      <th>State</th>\n",
       "      <th>Postal Code</th>\n",
       "      <th>Region</th>\n",
       "      <th>Product ID</th>\n",
       "      <th>Category</th>\n",
       "      <th>Sub-Category</th>\n",
       "      <th>Product Name</th>\n",
       "      <th>Sales</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>CA-2017-152156</td>\n",
       "      <td>08/11/2017</td>\n",
       "      <td>11/11/2017</td>\n",
       "      <td>Second Class</td>\n",
       "      <td>CG-12520</td>\n",
       "      <td>Claire Gute</td>\n",
       "      <td>Consumer</td>\n",
       "      <td>United States</td>\n",
       "      <td>Henderson</td>\n",
       "      <td>Kentucky</td>\n",
       "      <td>42420.0</td>\n",
       "      <td>South</td>\n",
       "      <td>FUR-BO-10001798</td>\n",
       "      <td>Furniture</td>\n",
       "      <td>Bookcases</td>\n",
       "      <td>Bush Somerset Collection Bookcase</td>\n",
       "      <td>261.9600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>CA-2017-152156</td>\n",
       "      <td>08/11/2017</td>\n",
       "      <td>11/11/2017</td>\n",
       "      <td>Second Class</td>\n",
       "      <td>CG-12520</td>\n",
       "      <td>Claire Gute</td>\n",
       "      <td>Consumer</td>\n",
       "      <td>United States</td>\n",
       "      <td>Henderson</td>\n",
       "      <td>Kentucky</td>\n",
       "      <td>42420.0</td>\n",
       "      <td>South</td>\n",
       "      <td>FUR-CH-10000454</td>\n",
       "      <td>Furniture</td>\n",
       "      <td>Chairs</td>\n",
       "      <td>Hon Deluxe Fabric Upholstered Stacking Chairs,...</td>\n",
       "      <td>731.9400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>CA-2017-138688</td>\n",
       "      <td>12/06/2017</td>\n",
       "      <td>16/06/2017</td>\n",
       "      <td>Second Class</td>\n",
       "      <td>DV-13045</td>\n",
       "      <td>Darrin Van Huff</td>\n",
       "      <td>Corporate</td>\n",
       "      <td>United States</td>\n",
       "      <td>Los Angeles</td>\n",
       "      <td>California</td>\n",
       "      <td>90036.0</td>\n",
       "      <td>West</td>\n",
       "      <td>OFF-LA-10000240</td>\n",
       "      <td>Office Supplies</td>\n",
       "      <td>Labels</td>\n",
       "      <td>Self-Adhesive Address Labels for Typewriters b...</td>\n",
       "      <td>14.6200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>US-2016-108966</td>\n",
       "      <td>11/10/2016</td>\n",
       "      <td>18/10/2016</td>\n",
       "      <td>Standard Class</td>\n",
       "      <td>SO-20335</td>\n",
       "      <td>Sean O'Donnell</td>\n",
       "      <td>Consumer</td>\n",
       "      <td>United States</td>\n",
       "      <td>Fort Lauderdale</td>\n",
       "      <td>Florida</td>\n",
       "      <td>33311.0</td>\n",
       "      <td>South</td>\n",
       "      <td>FUR-TA-10000577</td>\n",
       "      <td>Furniture</td>\n",
       "      <td>Tables</td>\n",
       "      <td>Bretford CR4500 Series Slim Rectangular Table</td>\n",
       "      <td>957.5775</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>US-2016-108966</td>\n",
       "      <td>11/10/2016</td>\n",
       "      <td>18/10/2016</td>\n",
       "      <td>Standard Class</td>\n",
       "      <td>SO-20335</td>\n",
       "      <td>Sean O'Donnell</td>\n",
       "      <td>Consumer</td>\n",
       "      <td>United States</td>\n",
       "      <td>Fort Lauderdale</td>\n",
       "      <td>Florida</td>\n",
       "      <td>33311.0</td>\n",
       "      <td>South</td>\n",
       "      <td>OFF-ST-10000760</td>\n",
       "      <td>Office Supplies</td>\n",
       "      <td>Storage</td>\n",
       "      <td>Eldon Fold 'N Roll Cart System</td>\n",
       "      <td>22.3680</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Row ID        Order ID  Order Date   Ship Date       Ship Mode Customer ID  \\\n",
       "0       1  CA-2017-152156  08/11/2017  11/11/2017    Second Class    CG-12520   \n",
       "1       2  CA-2017-152156  08/11/2017  11/11/2017    Second Class    CG-12520   \n",
       "2       3  CA-2017-138688  12/06/2017  16/06/2017    Second Class    DV-13045   \n",
       "3       4  US-2016-108966  11/10/2016  18/10/2016  Standard Class    SO-20335   \n",
       "4       5  US-2016-108966  11/10/2016  18/10/2016  Standard Class    SO-20335   \n",
       "\n",
       "     Customer Name    Segment        Country             City       State  \\\n",
       "0      Claire Gute   Consumer  United States        Henderson    Kentucky   \n",
       "1      Claire Gute   Consumer  United States        Henderson    Kentucky   \n",
       "2  Darrin Van Huff  Corporate  United States      Los Angeles  California   \n",
       "3   Sean O'Donnell   Consumer  United States  Fort Lauderdale     Florida   \n",
       "4   Sean O'Donnell   Consumer  United States  Fort Lauderdale     Florida   \n",
       "\n",
       "   Postal Code Region       Product ID         Category Sub-Category  \\\n",
       "0      42420.0  South  FUR-BO-10001798        Furniture    Bookcases   \n",
       "1      42420.0  South  FUR-CH-10000454        Furniture       Chairs   \n",
       "2      90036.0   West  OFF-LA-10000240  Office Supplies       Labels   \n",
       "3      33311.0  South  FUR-TA-10000577        Furniture       Tables   \n",
       "4      33311.0  South  OFF-ST-10000760  Office Supplies      Storage   \n",
       "\n",
       "                                        Product Name     Sales  \n",
       "0                  Bush Somerset Collection Bookcase  261.9600  \n",
       "1  Hon Deluxe Fabric Upholstered Stacking Chairs,...  731.9400  \n",
       "2  Self-Adhesive Address Labels for Typewriters b...   14.6200  \n",
       "3      Bretford CR4500 Series Slim Rectangular Table  957.5775  \n",
       "4                     Eldon Fold 'N Roll Cart System   22.3680  "
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load train.csv\n",
    "df_main = pd.read_csv(\"train.csv\")\n",
    "\n",
    "# Show basic info\n",
    "print(df_main.info())\n",
    "df_main.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b594f766-0eca-4e57-8a86-42e0b1b5f4e4",
   "metadata": {},
   "source": [
    "## Preprocess Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "657bfd8b-2c2b-4134-bd3b-edfb0f0c2aa0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Null postal codes handled.\n",
      "🧹 Duplicate rows before: 1\n",
      "✅ Duplicate rows after: 0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Order Date</th>\n",
       "      <th>Ship Date</th>\n",
       "      <th>Ship Mode</th>\n",
       "      <th>Customer Name</th>\n",
       "      <th>Segment</th>\n",
       "      <th>Country</th>\n",
       "      <th>City</th>\n",
       "      <th>State</th>\n",
       "      <th>Postal Code</th>\n",
       "      <th>Region</th>\n",
       "      <th>Category</th>\n",
       "      <th>Sub-Category</th>\n",
       "      <th>Product Name</th>\n",
       "      <th>Sales</th>\n",
       "      <th>Order_Year</th>\n",
       "      <th>Order_Month</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2017-11-08</td>\n",
       "      <td>2017-11-11</td>\n",
       "      <td>Second Class</td>\n",
       "      <td>Claire Gute</td>\n",
       "      <td>Consumer</td>\n",
       "      <td>United States</td>\n",
       "      <td>Henderson</td>\n",
       "      <td>Kentucky</td>\n",
       "      <td>42420</td>\n",
       "      <td>South</td>\n",
       "      <td>Furniture</td>\n",
       "      <td>Bookcases</td>\n",
       "      <td>Bush Somerset Collection Bookcase</td>\n",
       "      <td>261.9600</td>\n",
       "      <td>2017</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2017-11-08</td>\n",
       "      <td>2017-11-11</td>\n",
       "      <td>Second Class</td>\n",
       "      <td>Claire Gute</td>\n",
       "      <td>Consumer</td>\n",
       "      <td>United States</td>\n",
       "      <td>Henderson</td>\n",
       "      <td>Kentucky</td>\n",
       "      <td>42420</td>\n",
       "      <td>South</td>\n",
       "      <td>Furniture</td>\n",
       "      <td>Chairs</td>\n",
       "      <td>Hon Deluxe Fabric Upholstered Stacking Chairs,...</td>\n",
       "      <td>731.9400</td>\n",
       "      <td>2017</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2017-06-12</td>\n",
       "      <td>2017-06-16</td>\n",
       "      <td>Second Class</td>\n",
       "      <td>Darrin Van Huff</td>\n",
       "      <td>Corporate</td>\n",
       "      <td>United States</td>\n",
       "      <td>Los Angeles</td>\n",
       "      <td>California</td>\n",
       "      <td>90036</td>\n",
       "      <td>West</td>\n",
       "      <td>Office Supplies</td>\n",
       "      <td>Labels</td>\n",
       "      <td>Self-Adhesive Address Labels for Typewriters b...</td>\n",
       "      <td>14.6200</td>\n",
       "      <td>2017</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2016-10-11</td>\n",
       "      <td>2016-10-18</td>\n",
       "      <td>Standard Class</td>\n",
       "      <td>Sean O'Donnell</td>\n",
       "      <td>Consumer</td>\n",
       "      <td>United States</td>\n",
       "      <td>Fort Lauderdale</td>\n",
       "      <td>Florida</td>\n",
       "      <td>33311</td>\n",
       "      <td>South</td>\n",
       "      <td>Furniture</td>\n",
       "      <td>Tables</td>\n",
       "      <td>Bretford CR4500 Series Slim Rectangular Table</td>\n",
       "      <td>957.5775</td>\n",
       "      <td>2016</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2016-10-11</td>\n",
       "      <td>2016-10-18</td>\n",
       "      <td>Standard Class</td>\n",
       "      <td>Sean O'Donnell</td>\n",
       "      <td>Consumer</td>\n",
       "      <td>United States</td>\n",
       "      <td>Fort Lauderdale</td>\n",
       "      <td>Florida</td>\n",
       "      <td>33311</td>\n",
       "      <td>South</td>\n",
       "      <td>Office Supplies</td>\n",
       "      <td>Storage</td>\n",
       "      <td>Eldon Fold 'N Roll Cart System</td>\n",
       "      <td>22.3680</td>\n",
       "      <td>2016</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Order Date  Ship Date       Ship Mode    Customer Name    Segment  \\\n",
       "0 2017-11-08 2017-11-11    Second Class      Claire Gute   Consumer   \n",
       "1 2017-11-08 2017-11-11    Second Class      Claire Gute   Consumer   \n",
       "2 2017-06-12 2017-06-16    Second Class  Darrin Van Huff  Corporate   \n",
       "3 2016-10-11 2016-10-18  Standard Class   Sean O'Donnell   Consumer   \n",
       "4 2016-10-11 2016-10-18  Standard Class   Sean O'Donnell   Consumer   \n",
       "\n",
       "         Country             City       State  Postal Code Region  \\\n",
       "0  United States        Henderson    Kentucky        42420  South   \n",
       "1  United States        Henderson    Kentucky        42420  South   \n",
       "2  United States      Los Angeles  California        90036   West   \n",
       "3  United States  Fort Lauderdale     Florida        33311  South   \n",
       "4  United States  Fort Lauderdale     Florida        33311  South   \n",
       "\n",
       "          Category Sub-Category  \\\n",
       "0        Furniture    Bookcases   \n",
       "1        Furniture       Chairs   \n",
       "2  Office Supplies       Labels   \n",
       "3        Furniture       Tables   \n",
       "4  Office Supplies      Storage   \n",
       "\n",
       "                                        Product Name     Sales  Order_Year  \\\n",
       "0                  Bush Somerset Collection Bookcase  261.9600        2017   \n",
       "1  Hon Deluxe Fabric Upholstered Stacking Chairs,...  731.9400        2017   \n",
       "2  Self-Adhesive Address Labels for Typewriters b...   14.6200        2017   \n",
       "3      Bretford CR4500 Series Slim Rectangular Table  957.5775        2016   \n",
       "4                     Eldon Fold 'N Roll Cart System   22.3680        2016   \n",
       "\n",
       "   Order_Month  \n",
       "0           11  \n",
       "1           11  \n",
       "2            6  \n",
       "3           10  \n",
       "4           10  "
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1. Convert date columns to datetime\n",
    "df_main[\"Order Date\"] = pd.to_datetime(df_main[\"Order Date\"], dayfirst=True)\n",
    "df_main[\"Ship Date\"] = pd.to_datetime(df_main[\"Ship Date\"], dayfirst=True)\n",
    "\n",
    "# 2. Handle missing Postal Codes\n",
    "df_main[\"Postal Code\"] = df_main[\"Postal Code\"].fillna(-1).astype(int)\n",
    "print(\"✅ Null postal codes handled.\")\n",
    "\n",
    "# 3. Drop irrelevant columns\n",
    "df_main_cleaned = df_main.drop([\"Row ID\", \"Order ID\", \"Customer ID\", \"Product ID\"], axis=1)\n",
    "\n",
    "# 4. Extract year and month\n",
    "df_main_cleaned[\"Order_Year\"] = df_main_cleaned[\"Order Date\"].dt.year\n",
    "df_main_cleaned[\"Order_Month\"] = df_main_cleaned[\"Order Date\"].dt.month\n",
    "\n",
    "# 5. Check and remove duplicates\n",
    "dup_count_before = df_main_cleaned.duplicated().sum()\n",
    "print(f\"🧹 Duplicate rows before: {dup_count_before}\")\n",
    "df_main_cleaned = df_main_cleaned.drop_duplicates()\n",
    "dup_count_after = df_main_cleaned.duplicated().sum()\n",
    "print(f\"✅ Duplicate rows after: {dup_count_after}\")\n",
    "\n",
    "# 6. Preview cleaned dataset\n",
    "df_main_cleaned.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "171687f0-1895-43fa-9b36-30ba934e98b4",
   "metadata": {},
   "source": [
    "## 4. Load and Preview Holiday Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "bccee6a9-28ee-455f-b49b-3d33b1a778e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🗓️ Shape: (47, 3)\n",
      "🧠 Columns: ['date', 'holiday_name', 'holiday_type']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>holiday_name</th>\n",
       "      <th>holiday_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2015-01-01</td>\n",
       "      <td>New Year's Day</td>\n",
       "      <td>National holiday</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2015-01-19</td>\n",
       "      <td>Martin Luther King Jr. Day</td>\n",
       "      <td>National holiday</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2015-02-16</td>\n",
       "      <td>Presidents' Day</td>\n",
       "      <td>National holiday</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2015-05-25</td>\n",
       "      <td>Memorial Day</td>\n",
       "      <td>National holiday</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2015-07-03</td>\n",
       "      <td>Independence Day (substitute)</td>\n",
       "      <td>National holiday</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         date                   holiday_name      holiday_type\n",
       "0  2015-01-01                 New Year's Day  National holiday\n",
       "1  2015-01-19     Martin Luther King Jr. Day  National holiday\n",
       "2  2015-02-16                Presidents' Day  National holiday\n",
       "3  2015-05-25                   Memorial Day  National holiday\n",
       "4  2015-07-03  Independence Day (substitute)  National holiday"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load holidays data\n",
    "df_holidays = pd.read_csv(\"us_holidays_2015_2018.csv\")\n",
    "\n",
    "# Preview the dataset\n",
    "print(\"🗓️ Shape:\", df_holidays.shape)\n",
    "print(\"🧠 Columns:\", df_holidays.columns.tolist())\n",
    "df_holidays.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1159fec1-ed98-48e2-b8c4-46c19a202540",
   "metadata": {},
   "source": [
    "##  Preprocess Holiday Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "541c383d-c34a-45fe-8b61-6cc0a99650fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🧼 Null values:\n",
      " date            0\n",
      "holiday_name    0\n",
      "holiday_type    0\n",
      "dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>holiday_name</th>\n",
       "      <th>holiday_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2015-01-01</td>\n",
       "      <td>New Year's Day</td>\n",
       "      <td>National holiday</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2015-01-19</td>\n",
       "      <td>Martin Luther King Jr. Day</td>\n",
       "      <td>National holiday</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2015-02-16</td>\n",
       "      <td>Presidents' Day</td>\n",
       "      <td>National holiday</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2015-05-25</td>\n",
       "      <td>Memorial Day</td>\n",
       "      <td>National holiday</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2015-07-03</td>\n",
       "      <td>Independence Day (substitute)</td>\n",
       "      <td>National holiday</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        date                   holiday_name      holiday_type\n",
       "0 2015-01-01                 New Year's Day  National holiday\n",
       "1 2015-01-19     Martin Luther King Jr. Day  National holiday\n",
       "2 2015-02-16                Presidents' Day  National holiday\n",
       "3 2015-05-25                   Memorial Day  National holiday\n",
       "4 2015-07-03  Independence Day (substitute)  National holiday"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# 1. Convert 'date' column to datetime format\n",
    "df_holidays['date'] = pd.to_datetime(df_holidays['date'])\n",
    "\n",
    "# 2. Standardize column names (optional but useful)\n",
    "df_holidays.columns = df_holidays.columns.str.strip().str.lower().str.replace(\" \", \"_\")\n",
    "\n",
    "# 3. Remove any duplicate rows (if any)\n",
    "df_holidays = df_holidays.drop_duplicates()\n",
    "\n",
    "# 4. Check for null values\n",
    "print(\"🧼 Null values:\\n\", df_holidays.isnull().sum())\n",
    "\n",
    "# 5. Final preview\n",
    "df_holidays.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ad87f20-2d63-4d20-bff8-5c7181b9c2a0",
   "metadata": {},
   "source": [
    "## 5. Load and Inspect COVID-19 Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "3ef78193-d855-4ece-9ac8-46a8dd2daec5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 473197 entries, 0 to 473196\n",
      "Data columns (total 3 columns):\n",
      " #   Column                                                                                                Non-Null Count   Dtype  \n",
      "---  ------                                                                                                --------------   -----  \n",
      " 0   Entity                                                                                                473197 non-null  object \n",
      " 1   Day                                                                                                   473197 non-null  object \n",
      " 2   Daily new confirmed deaths due to COVID-19 per million people (rolling 7-day average, right-aligned)  473197 non-null  float64\n",
      "dtypes: float64(1), object(2)\n",
      "memory usage: 10.8+ MB\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Entity</th>\n",
       "      <th>Day</th>\n",
       "      <th>Daily new confirmed deaths due to COVID-19 per million people (rolling 7-day average, right-aligned)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>2020-01-09</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>2020-01-10</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>2020-01-11</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>2020-01-12</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>2020-01-13</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Entity         Day  \\\n",
       "0  Afghanistan  2020-01-09   \n",
       "1  Afghanistan  2020-01-10   \n",
       "2  Afghanistan  2020-01-11   \n",
       "3  Afghanistan  2020-01-12   \n",
       "4  Afghanistan  2020-01-13   \n",
       "\n",
       "   Daily new confirmed deaths due to COVID-19 per million people (rolling 7-day average, right-aligned)  \n",
       "0                                                0.0                                                     \n",
       "1                                                0.0                                                     \n",
       "2                                                0.0                                                     \n",
       "3                                                0.0                                                     \n",
       "4                                                0.0                                                     "
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df_covid = pd.read_csv(\"daily-new-confirmed-covid-19-deaths-per-million-people (2).csv\")\n",
    "df_covid.info()\n",
    "df_covid.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "41277d6a-a0a9-49d3-8516-91fab5d31b8c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Deaths_per_million</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2020-01-09</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2020-01-10</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2020-01-11</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2020-01-12</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2020-01-13</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Date  Deaths_per_million\n",
       "0 2020-01-09                 0.0\n",
       "1 2020-01-10                 0.0\n",
       "2 2020-01-11                 0.0\n",
       "3 2020-01-12                 0.0\n",
       "4 2020-01-13                 0.0"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Rename columns for simplicity\n",
    "df_covid.columns = ['Country', 'Date', 'Deaths_per_million']\n",
    "\n",
    "# Convert date to datetime format\n",
    "df_covid['Date'] = pd.to_datetime(df_covid['Date'])\n",
    "\n",
    "# Filter data for the United States\n",
    "df_covid_usa = df_covid[df_covid['Country'] == 'United States']\n",
    "\n",
    "# Reset index and drop the original country column (if not needed later)\n",
    "df_covid_usa = df_covid_usa[['Date', 'Deaths_per_million']].reset_index(drop=True)\n",
    "\n",
    "df_covid_usa.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68f371dd-4003-4914-bd46-0e0705dd2e00",
   "metadata": {},
   "source": [
    "## 6. Load and Inspect Trends Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "7e695d3a-7d52-4700-9935-4246e6654e77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 26955 entries, 0 to 26954\n",
      "Data columns (total 5 columns):\n",
      " #   Column    Non-Null Count  Dtype \n",
      "---  ------    --------------  ----- \n",
      " 0   location  26955 non-null  object\n",
      " 1   year      26955 non-null  int64 \n",
      " 2   category  26955 non-null  object\n",
      " 3   rank      26955 non-null  int64 \n",
      " 4   query     26955 non-null  object\n",
      "dtypes: int64(2), object(3)\n",
      "memory usage: 1.0+ MB\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>location</th>\n",
       "      <th>year</th>\n",
       "      <th>category</th>\n",
       "      <th>rank</th>\n",
       "      <th>query</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Global</td>\n",
       "      <td>2001</td>\n",
       "      <td>Consumer Brands</td>\n",
       "      <td>1</td>\n",
       "      <td>Nokia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Global</td>\n",
       "      <td>2001</td>\n",
       "      <td>Consumer Brands</td>\n",
       "      <td>2</td>\n",
       "      <td>Sony</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Global</td>\n",
       "      <td>2001</td>\n",
       "      <td>Consumer Brands</td>\n",
       "      <td>3</td>\n",
       "      <td>BMW</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Global</td>\n",
       "      <td>2001</td>\n",
       "      <td>Consumer Brands</td>\n",
       "      <td>4</td>\n",
       "      <td>Palm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Global</td>\n",
       "      <td>2001</td>\n",
       "      <td>Consumer Brands</td>\n",
       "      <td>5</td>\n",
       "      <td>Adobe</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  location  year         category  rank  query\n",
       "0   Global  2001  Consumer Brands     1  Nokia\n",
       "1   Global  2001  Consumer Brands     2   Sony\n",
       "2   Global  2001  Consumer Brands     3    BMW\n",
       "3   Global  2001  Consumer Brands     4   Palm\n",
       "4   Global  2001  Consumer Brands     5  Adobe"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_trends = pd.read_csv(\"trends.csv\")\n",
    "df_trends.info()\n",
    "df_trends.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e25a4c6-fcdb-4529-a05e-91c24ecc836f",
   "metadata": {},
   "source": [
    "##  Preprocess Trends Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "708e12df-6886-4cc8-9ec3-9f8e66446936",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Movie Showtimes' 'Scandals' 'Tickets' 'Deaths of 2007' 'Diets' 'Fitness'\n",
      " 'How To' 'Lawsuits' 'Lyrics' 'Presidential Campaign' 'Recipes'\n",
      " 'Ringtones' 'What is' 'Who is' \"It's the Economy\" 'Books'\n",
      " 'Google.com Fastest Falling' 'Google.com Fastest Rising' 'Images'\n",
      " 'Maps Searches' 'Mobile Searches' 'News Searches' 'Baseball' 'Basketball'\n",
      " 'Cars' 'Costumes' 'Dance Moves' 'DIY' 'Donate To' 'Economy'\n",
      " 'Falling Searches' 'Football' 'Gadgets' 'Google Image Searches'\n",
      " 'Google Maps Searches' 'Google News Searches' 'Home Appliances'\n",
      " 'Literature' 'News Sources' 'Political Scandals' 'Politics'\n",
      " 'Pop Culture / People' 'Pop Culture / Songs' 'Pop Culture / Toys'\n",
      " 'Pop Culture / TV Shows' 'Product Searches' 'Quirky Environmental '\n",
      " 'Recalls' 'Science' 'Soccer' 'Sports' 'Technology' 'Top Trending'\n",
      " 'Workouts' 'Annual Events of 2012' 'Apparel Brands' 'Athletes' 'Authors'\n",
      " 'Calorie Searches' 'Colleges & Universities' 'Donations'\n",
      " 'Election Issues' 'Endangered Species' 'Events of 2012' 'Food Delivery'\n",
      " 'Google Doodles of 2012' 'Halloween Costumes' 'Health Issues' 'How to...'\n",
      " 'Hybrid Card' 'Jewelry Brands' 'Mobile\\\\\\\\Tablet Apps' 'Museums'\n",
      " 'NFL Teams' 'People' 'Phones' 'Political Gaffes' 'Reality TV Stars'\n",
      " 'Searches' 'Shoe Brands' 'Song Lyric Searches' 'Stocks' 'Symptoms'\n",
      " 'Tech Gadgets' 'Toys' 'TV Shows' 'US Politicians' 'Video Games'\n",
      " 'What is...?' 'Annual Events' 'Appetizers' 'Atlanta, GA' 'Austin, TX'\n",
      " 'Beer' 'Blogs' 'Boston / Cambridge, MA' 'Boy Names' 'Car Companies'\n",
      " 'Chicago, IL' 'Cocktails' 'Colleges and Universities' 'Dallas, TX'\n",
      " 'Denver, CO' 'Des Moines, IA' 'Destination Wedding Locations'\n",
      " 'Detroit, MI' 'Events' 'Famous Internet Animals' 'Female Politicians'\n",
      " 'Girl Names' 'Honeymoon Locations' 'How To...' 'Hybrid and Electric Cars'\n",
      " 'Kansas City, KS' 'Los Angeles, CA' 'Male Politicians' 'Memes'\n",
      " 'Miami, FL' 'MLB Teams' 'Mobile/Tablet Apps' 'NBA Teams'\n",
      " 'New Cars, Trending' 'New York City, NY' 'Oklahoma City, OK'\n",
      " 'People, Trending' 'Philadelphia, PA' 'Portland, OR' 'Sacramento, CA'\n",
      " 'San Francisco, CA' 'Seattle, WA' 'Social Media Sites' 'Song Lyrics'\n",
      " 'TV Series Finales' 'TV Shows, Trending' 'Washington, D.C.' 'Weddings'\n",
      " '.gifs' 'Actors' 'Actresses' 'Celebrity Pregnancies'\n",
      " 'Celebrity Weddings of 2014' 'Fashion Designers' 'Google Doodles of 2014'\n",
      " 'How to...?' \"IPO's of 2014\" 'Losses of 2014' 'MLB Players' 'Movies'\n",
      " 'Music Artists' 'Natural Events' 'NBA Players' 'NFL Players' 'Podcasts'\n",
      " 'Red Carpet Appearances' 'Selfies' 'Sports Teams'\n",
      " 'Workouts and Exercises' 'World Cup Matches' 'Beauty Questions'\n",
      " 'Car Models' 'Celebrity Weddings' 'Dog Questions' 'Fashion Questions'\n",
      " 'GIF' 'Losses' \"Men's Soccer Players\" 'Oscar Red Carpet Dresses'\n",
      " 'Politicians' 'Soccer Teams' 'Travel Questions' 'WNBA Teams'\n",
      " \"Women's Soccer Players\" 'News' 'Car Brands' 'Consumer Tech'\n",
      " 'Election Moments' 'Election Questions' 'GIFs' 'MLS Soccer Teams'\n",
      " 'Musicians' 'NHL Players' 'NHL Teams' 'Olympic Athletes' 'Olympic Events'\n",
      " 'Olympic Moments' 'Soccer Teams (all)' 'Songs' 'What is...'\n",
      " 'WNBA Players' 'Activations (protests)' 'Calories' 'Dog Breeds'\n",
      " 'Musicians and Bands' 'Professional Sports Teams' 'Songs/Lyrics'\n",
      " '¿Cómo...?' '¿Dónde...?' 'Equipos de fútbol' 'Películas' '¿Qué es...?'\n",
      " 'Recetas' 'Diet' 'Fashion Brands' 'Fashion Searches' 'Food' 'How to'\n",
      " 'Loss' 'Where is...?' 'Who...?' 'World Cup Teams' 'Cómo... ?' 'Dónde...?'\n",
      " 'Qué es...?' 'Canciones' 'Babies' 'Fashion Style searches'\n",
      " 'Home Style searches' 'Outfit ideas' 'Passings'\n",
      " 'People of the Red Carpet' 'Professional Sports Scores' 'Trip to...'\n",
      " 'Como...' 'Donde...' 'Peliculas' 'Que es...' \"Beauty how to's\"\n",
      " 'Definitions' 'Games' 'How to donate...' 'How to help...'\n",
      " 'How to make...' 'How to style...' 'Near me' 'Virtual' 'Where is...'\n",
      " 'Where to buy...' 'Why...' '... during coronavirus' 'Cómo...' 'Dónde...'\n",
      " 'Qué es...' 'Quién...']\n",
      "year\n",
      "2006     15\n",
      "2007     55\n",
      "2008      5\n",
      "2009     40\n",
      "2011    165\n",
      "2012    195\n",
      "2013    330\n",
      "2014    200\n",
      "2015    225\n",
      "2016    255\n",
      "2017    135\n",
      "2018    150\n",
      "2019    135\n",
      "2020    165\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Clean column names\n",
    "df_trends.columns = df_trends.columns.str.lower().str.replace(' ', '_')\n",
    "\n",
    "# Step 2: Drop duplicates\n",
    "df_trends.drop_duplicates(inplace=True)\n",
    "\n",
    "# Step 3: Filter for United States\n",
    "df_trends_us = df_trends[df_trends['location'] == 'United States'].copy()\n",
    "\n",
    "# Optional: Check unique categories or year ranges\n",
    "print(df_trends_us['category'].unique())\n",
    "print(df_trends_us['year'].value_counts().sort_index())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "e7cf4498-e480-4a14-8220-82e29d0c86b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_trends_us['category'] = df_trends_us['category'].str.strip().str.lower()\n",
    "\n",
    "# Example replacements (you can expand this dict as needed)\n",
    "replace_map = {\n",
    "    'how to...': 'how to',\n",
    "    'how to': 'how to',\n",
    "    'what is...?': 'what is',\n",
    "    'what is...': 'what is',\n",
    "    'gif': 'gifs',\n",
    "    'gifs': 'gifs',\n",
    "    'songs/lyrics': 'songs',\n",
    "    '¿qué es...?': 'what is',\n",
    "    'qué es...?': 'what is',\n",
    "    'fashion style searches': 'fashion',\n",
    "    'fashion questions': 'fashion',\n",
    "    'fashion brands': 'fashion',\n",
    "}\n",
    "\n",
    "df_trends_us['category'] = df_trends_us['category'].replace(replace_map)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "968910ac-b0d6-42e5-95e9-396f0aaaf724",
   "metadata": {},
   "source": [
    "## 7. Load and Inspect Reddit Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "df514c24-9976-47a8-b9c4-4bec70999365",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 37249 entries, 0 to 37248\n",
      "Data columns (total 2 columns):\n",
      " #   Column         Non-Null Count  Dtype \n",
      "---  ------         --------------  ----- \n",
      " 0   clean_comment  37149 non-null  object\n",
      " 1   category       37249 non-null  int64 \n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 582.1+ KB\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>clean_comment</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>family mormon have never tried explain them t...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>buddhism has very much lot compatible with chr...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>seriously don say thing first all they won get...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>what you have learned yours and only yours wha...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>for your own benefit you may want read living ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       clean_comment  category\n",
       "0   family mormon have never tried explain them t...         1\n",
       "1  buddhism has very much lot compatible with chr...         1\n",
       "2  seriously don say thing first all they won get...        -1\n",
       "3  what you have learned yours and only yours wha...         0\n",
       "4  for your own benefit you may want read living ...         1"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_reddit=pd.read_csv(\"Reddit_Data.csv\")\n",
    "df_reddit.info()\n",
    "df_reddit.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d61b3c5-3323-480b-9186-1c28727e7914",
   "metadata": {},
   "source": [
    "## Preprocess Reddit Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "1887f4d3-bb8a-4488-b80c-3ff43823cf7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "category\n",
      " 1    15830\n",
      " 0    13042\n",
      "-1     8277\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Drop rows where comment is missing\n",
    "df_reddit_cleaned = df_reddit.dropna(subset=[\"clean_comment\"])\n",
    "\n",
    "# Optional: Clean extra whitespace, lowercase, etc.\n",
    "df_reddit_cleaned.loc[:, \"clean_comment\"] = df_reddit_cleaned[\"clean_comment\"].str.strip().str.lower()\n",
    "\n",
    "\n",
    "# Optional: Check distribution of categories\n",
    "category_counts = df_reddit_cleaned[\"category\"].value_counts()\n",
    "print(category_counts)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "438e40c9-b7c9-4a80-acb8-567f4f87ce4f",
   "metadata": {},
   "source": [
    "## 8. Load and Inspect Twitter Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "1fa613b7-2475-4bad-8122-dd121117c0e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 162980 entries, 0 to 162979\n",
      "Data columns (total 2 columns):\n",
      " #   Column      Non-Null Count   Dtype  \n",
      "---  ------      --------------   -----  \n",
      " 0   clean_text  162976 non-null  object \n",
      " 1   category    162973 non-null  float64\n",
      "dtypes: float64(1), object(1)\n",
      "memory usage: 2.5+ MB\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>clean_text</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>when modi promised “minimum government maximum...</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>talk all the nonsense and continue all the dra...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>what did just say vote for modi  welcome bjp t...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>asking his supporters prefix chowkidar their n...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>answer who among these the most powerful world...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          clean_text  category\n",
       "0  when modi promised “minimum government maximum...      -1.0\n",
       "1  talk all the nonsense and continue all the dra...       0.0\n",
       "2  what did just say vote for modi  welcome bjp t...       1.0\n",
       "3  asking his supporters prefix chowkidar their n...       1.0\n",
       "4  answer who among these the most powerful world...       1.0"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_tweets=pd.read_csv(\"Twitter_Data.csv\")\n",
    "df_tweets.info()\n",
    "df_tweets.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13d1a89c-0c39-4e11-8132-7fd9b711bf79",
   "metadata": {},
   "source": [
    "##  Preprocess Twitter Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "e72733e1-fa89-4a59-9e8a-b051ce476ab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop missing\n",
    "df_tweets_cleaned = df_tweets.dropna(subset=[\"clean_text\", \"category\"]).copy()\n",
    "\n",
    "# Convert category to int\n",
    "df_tweets_cleaned[\"category\"] = df_tweets_cleaned[\"category\"].astype(int)\n",
    "\n",
    "# Clean text: strip + lowercase\n",
    "df_tweets_cleaned[\"clean_text\"] = df_tweets_cleaned[\"clean_text\"].str.strip().str.lower()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2975c7f1-c37a-4aa8-934e-a666f5fddd81",
   "metadata": {},
   "source": [
    "##  Combine and Process Sentiment Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "82c89024-c123-447f-9c29-428a8137f0e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 200118 entries, 0 to 200117\n",
      "Data columns (total 2 columns):\n",
      " #   Column    Non-Null Count   Dtype \n",
      "---  ------    --------------   ----- \n",
      " 0   text      200118 non-null  object\n",
      " 1   category  200118 non-null  int32 \n",
      "dtypes: int32(1), object(1)\n",
      "memory usage: 2.3+ MB\n",
      "None\n",
      "category\n",
      " 1    88079\n",
      " 0    68253\n",
      "-1    43786\n",
      "Name: count, dtype: int64\n",
      "                                                text  category\n",
      "0  family mormon have never tried explain them th...         1\n",
      "1  buddhism has very much lot compatible with chr...         1\n",
      "2  seriously don say thing first all they won get...        -1\n",
      "3  what you have learned yours and only yours wha...         0\n",
      "4  for your own benefit you may want read living ...         1\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# ✅ Step 1: Rename columns safely by creating new DataFrames\n",
    "df_reddit_cleaned = df_reddit_cleaned.rename(columns={\"clean_comment\": \"text\"})\n",
    "df_tweets_cleaned = df_tweets_cleaned.rename(columns={\"clean_text\": \"text\"})\n",
    "\n",
    "# ✅ Step 2: Convert 'category' to numeric (force errors to NaN), then drop any NaNs\n",
    "df_reddit_cleaned[\"category\"] = pd.to_numeric(df_reddit_cleaned[\"category\"], errors=\"coerce\")\n",
    "df_tweets_cleaned[\"category\"] = pd.to_numeric(df_tweets_cleaned[\"category\"], errors=\"coerce\")\n",
    "\n",
    "df_reddit_cleaned = df_reddit_cleaned.dropna(subset=[\"text\", \"category\"])\n",
    "df_tweets_cleaned = df_tweets_cleaned.dropna(subset=[\"text\", \"category\"])\n",
    "\n",
    "# ✅ Step 3: Convert 'category' to integer type explicitly\n",
    "df_reddit_cleaned[\"category\"] = df_reddit_cleaned[\"category\"].astype(int)\n",
    "df_tweets_cleaned[\"category\"] = df_tweets_cleaned[\"category\"].astype(int)\n",
    "\n",
    "# ✅ Step 4: Concatenate both cleaned datasets into one\n",
    "df_sentiment_combined = pd.concat(\n",
    "    [df_reddit_cleaned[[\"text\", \"category\"]],\n",
    "     df_tweets_cleaned[[\"text\", \"category\"]]],\n",
    "    ignore_index=True\n",
    ")\n",
    "\n",
    "# ✅ Step 5: Overview\n",
    "print(df_sentiment_combined.info())\n",
    "print(df_sentiment_combined[\"category\"].value_counts())\n",
    "print(df_sentiment_combined.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52a53cb3-1caf-4ca3-90ad-fa2ac037517e",
   "metadata": {},
   "source": [
    "##  Merge Holiday Data with Main Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "30bd1fef-3f92-4de1-b828-3f576575a97d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Merged shape: (9799, 18)\n",
      "     Order Date holiday_name holiday_type\n",
      "444  2017-09-05   No Holiday   No Holiday\n",
      "8547 2016-12-05   No Holiday   No Holiday\n",
      "3321 2015-12-20   No Holiday   No Holiday\n",
      "8337 2015-12-27   No Holiday   No Holiday\n",
      "3095 2016-08-23   No Holiday   No Holiday\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 2. Merge with main dataset on Order Date and Holiday date\n",
    "df_merged = df_main_cleaned.merge(\n",
    "    df_holidays,\n",
    "    how=\"left\",\n",
    "    left_on=\"Order Date\",\n",
    "    right_on=\"date\"\n",
    ")\n",
    "\n",
    "# 3. Drop the 'date' column after merge (already have Order Date)\n",
    "df_merged.drop(columns=[\"date\"], inplace=True)\n",
    "\n",
    "# 4. Fill missing holiday names/types with \"No Holiday\"\n",
    "df_merged[\"holiday_name\"] = df_merged[\"holiday_name\"].fillna(\"No Holiday\")\n",
    "df_merged[\"holiday_type\"] = df_merged[\"holiday_type\"].fillna(\"No Holiday\")\n",
    "\n",
    "# 5. Final check\n",
    "print(\"✅ Merged shape:\", df_merged.shape)\n",
    "print(df_merged[[\"Order Date\", \"holiday_name\", \"holiday_type\"]].sample(5))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9df9906-a48e-4e9d-a930-cd137ab54792",
   "metadata": {},
   "source": [
    "##  Extract Cities and Date Range from Main Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "ef65f48f-eba5-428c-b445-62616a1d0d97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📍 Total cities: 529\n",
      "['Henderson' 'Los Angeles' 'Fort Lauderdale' 'Concord' 'Seattle'\n",
      " 'Fort Worth' 'Madison' 'West Jordan' 'San Francisco' 'Fremont']\n",
      "📆 Date range: 2015-01-03 to 2018-12-30\n"
     ]
    }
   ],
   "source": [
    "# Extract unique cities from your main dataset\n",
    "unique_cities = df_main_cleaned['City'].dropna().unique()\n",
    "print(f\"📍 Total cities: {len(unique_cities)}\")\n",
    "print(unique_cities[:10])  # Preview first few cities\n",
    "\n",
    "# Get date range\n",
    "min_date = df_main_cleaned['Order Date'].min().strftime('%Y-%m-%d')\n",
    "max_date = df_main_cleaned['Order Date'].max().strftime('%Y-%m-%d')\n",
    "print(f\"📆 Date range: {min_date} to {max_date}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be21a468-6ec5-4013-9f88-69d9164f50f0",
   "metadata": {},
   "source": [
    "##  Identify Top 20 Cities by Order Frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "a3cda278-698f-40f6-9e16-62134ce6890c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['New York City', 'Los Angeles', 'Philadelphia', 'San Francisco', 'Seattle', 'Houston', 'Chicago', 'Columbus', 'San Diego', 'Springfield', 'Dallas', 'Jacksonville', 'Detroit', 'Newark', 'Jackson', 'Columbia', 'Richmond', 'Aurora', 'Phoenix', 'Arlington']\n"
     ]
    }
   ],
   "source": [
    "# Top 20 cities by frequency in order data\n",
    "top_cities = df_main_cleaned['City'].value_counts().head(20).index.tolist()\n",
    "print(top_cities)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c7ac029-1607-43bc-b23a-ad4b12f4ae0f",
   "metadata": {},
   "source": [
    "##  List DataFrame Names and Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "0089f20a-7fbe-47f3-b3c5-730a17027a5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_13: ['Row ID', 'Order ID', 'Order Date', 'Ship Date', 'Ship Mode', 'Customer ID', 'Customer Name', 'Segment', 'Country', 'City', 'State', 'Postal Code', 'Region', 'Product ID', 'Category', 'Sub-Category', 'Product Name', 'Sales']\n",
      "_15: ['Order Date', 'Ship Date', 'Ship Mode', 'Customer Name', 'Segment', 'Country', 'City', 'State', 'Postal Code', 'Region', 'Category', 'Sub-Category', 'Product Name', 'Sales', 'Order_Year', 'Order_Month']\n",
      "_17: ['date', 'holiday_name', 'holiday_type']\n",
      "_19: ['date', 'holiday_name', 'holiday_type']\n",
      "_44: ['date', 'holiday_name', 'holiday_type']\n",
      "_51: ['Row ID', 'Order ID', 'Order Date', 'Ship Date', 'Ship Mode', 'Customer ID', 'Customer Name', 'Segment', 'Country', 'City', 'State', 'Postal Code', 'Region', 'Product ID', 'Category', 'Sub-Category', 'Product Name', 'Sales']\n",
      "_53: ['Order Date', 'Ship Date', 'Ship Mode', 'Customer Name', 'Segment', 'Country', 'City', 'State', 'Postal Code', 'Region', 'Category', 'Sub-Category', 'Product Name', 'Sales', 'Order_Year', 'Order_Month']\n",
      "_55: ['date', 'holiday_name', 'holiday_type']\n",
      "_57: ['date', 'holiday_name', 'holiday_type']\n",
      "_59: ['Entity', 'Day', 'Daily new confirmed deaths due to COVID-19 per million people (rolling 7-day average, right-aligned)']\n",
      "_6: ['date', 'holiday_name', 'holiday_type']\n",
      "_60: ['Date', 'Deaths_per_million']\n",
      "_62: ['location', 'year', 'category', 'rank', 'query']\n",
      "_67: ['clean_comment', 'category']\n",
      "_71: ['clean_text', 'category']\n",
      "cpi_cleaned: ['Year', 'Month', 'CPI', 'Date']\n",
      "cpi_long: ['Year', 'Month', 'CPI']\n",
      "cpi_table: ['Year', 'Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', 'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec']\n",
      "df: ['date', 'holiday_name', 'holiday_type']\n",
      "df_covid: ['Country', 'Date', 'Deaths_per_million']\n",
      "df_covid_usa: ['Date', 'Deaths_per_million']\n",
      "df_holidays: ['date', 'holiday_name', 'holiday_type']\n",
      "df_main: ['Row ID', 'Order ID', 'Order Date', 'Ship Date', 'Ship Mode', 'Customer ID', 'Customer Name', 'Segment', 'Country', 'City', 'State', 'Postal Code', 'Region', 'Product ID', 'Category', 'Sub-Category', 'Product Name', 'Sales']\n",
      "df_main_cleaned: ['Order Date', 'Ship Date', 'Ship Mode', 'Customer Name', 'Segment', 'Country', 'City', 'State', 'Postal Code', 'Region', 'Category', 'Sub-Category', 'Product Name', 'Sales', 'Order_Year', 'Order_Month']\n",
      "df_merged: ['Order Date', 'Ship Date', 'Ship Mode', 'Customer Name', 'Segment', 'Country', 'City', 'State', 'Postal Code', 'Region', 'Category', 'Sub-Category', 'Product Name', 'Sales', 'Order_Year', 'Order_Month', 'holiday_name', 'holiday_type']\n",
      "df_reddit: ['clean_comment', 'category']\n",
      "df_reddit_cleaned: ['text', 'category']\n",
      "df_sentiment_combined: ['text', 'category']\n",
      "df_trends: ['location', 'year', 'category', 'rank', 'query']\n",
      "df_trends_us: ['location', 'year', 'category', 'rank', 'query']\n",
      "df_tweets: ['clean_text', 'category']\n",
      "df_tweets_cleaned: ['text', 'category']\n",
      "holidays_df: ['date', 'holiday_name', 'holiday_type']\n",
      "table: ['Year', 'Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', 'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec', 'Ave.']\n"
     ]
    }
   ],
   "source": [
    "for name in dir():\n",
    "    if isinstance(eval(name), pd.DataFrame):\n",
    "        print(f\"{name}: {eval(name).columns.tolist()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bff09b3-6fca-4e9c-9b47-bbbfa06c4681",
   "metadata": {},
   "source": [
    "##  Convert Order Date to datetime and Add Year Column\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "50ab7875-35f6-4e29-a8a6-0b6c6a1b75ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert 'Order Date' to datetime\n",
    "df_merged[\"Order Date\"] = pd.to_datetime(df_merged[\"Order Date\"])\n",
    "\n",
    "# Add year column\n",
    "df_merged[\"year\"] = df_merged[\"Order Date\"].dt.year\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb719703-7664-476b-90a3-f5173348a2af",
   "metadata": {},
   "source": [
    "##  Convert Year Column to Integer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "926a1013-e559-4738-8ace-c15a7490536e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_trends_us[\"year\"] = df_trends_us[\"year\"].astype(int)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f8f83c7-19f3-4bf3-b449-29b0ce9a6d98",
   "metadata": {},
   "source": [
    "##  Merge Main and Trends Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "f5538d24-4cee-4a25-84fb-7a402e994876",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final = pd.merge(df_merged, df_trends_us, on=\"year\", how=\"left\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e057a83-d94f-4894-bafc-931a8766dc05",
   "metadata": {},
   "source": [
    "##  Inspect Merged DataFrame\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "0b43982b-9ec0-4536-8a7a-f1c8a3df7d14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1794015 entries, 0 to 1794014\n",
      "Data columns (total 23 columns):\n",
      " #   Column         Dtype         \n",
      "---  ------         -----         \n",
      " 0   Order Date     datetime64[ns]\n",
      " 1   Ship Date      datetime64[ns]\n",
      " 2   Ship Mode      object        \n",
      " 3   Customer Name  object        \n",
      " 4   Segment        object        \n",
      " 5   Country        object        \n",
      " 6   City           object        \n",
      " 7   State          object        \n",
      " 8   Postal Code    int32         \n",
      " 9   Region         object        \n",
      " 10  Category       object        \n",
      " 11  Sub-Category   object        \n",
      " 12  Product Name   object        \n",
      " 13  Sales          float64       \n",
      " 14  Order_Year     int32         \n",
      " 15  Order_Month    int32         \n",
      " 16  holiday_name   object        \n",
      " 17  holiday_type   object        \n",
      " 18  year           int32         \n",
      " 19  location       object        \n",
      " 20  category       object        \n",
      " 21  rank           int64         \n",
      " 22  query          object        \n",
      "dtypes: datetime64[ns](2), float64(1), int32(4), int64(1), object(15)\n",
      "memory usage: 287.4+ MB\n"
     ]
    }
   ],
   "source": [
    "df_final.head()\n",
    "df_final.columns\n",
    "df_final.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "c2b1258d-5bb6-4e47-9465-6fbb855fc189",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Order Date       0\n",
       "Ship Date        0\n",
       "Ship Mode        0\n",
       "Customer Name    0\n",
       "Segment          0\n",
       "Country          0\n",
       "City             0\n",
       "State            0\n",
       "Postal Code      0\n",
       "Region           0\n",
       "Category         0\n",
       "Sub-Category     0\n",
       "Product Name     0\n",
       "Sales            0\n",
       "Order_Year       0\n",
       "Order_Month      0\n",
       "holiday_name     0\n",
       "holiday_type     0\n",
       "year             0\n",
       "location         0\n",
       "category         0\n",
       "rank             0\n",
       "query            0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_final.isnull().sum()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "153650f7-0cd2-4e67-9973-d3a2eff9b6c3",
   "metadata": {},
   "source": [
    "##  Merge COVID-19 Data with Main DataFrame\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "e8880345-6f15-4f56-940f-2ac55325f958",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Create a new column 'Order_Date_only' to match COVID Date granularity\n",
    "df_final['Order_Date_only'] = df_final['Order Date'].dt.date\n",
    "\n",
    "# Step 2: Also extract date only (not datetime) in df_covid_usa\n",
    "df_covid_usa['Date_only'] = df_covid_usa['Date'].dt.date\n",
    "\n",
    "# Step 3: Merge on date\n",
    "df_final = df_final.merge(\n",
    "    df_covid_usa[['Date_only', 'Deaths_per_million']],\n",
    "    left_on='Order_Date_only',\n",
    "    right_on='Date_only',\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# Step 4: Drop helper columns if you want\n",
    "df_final.drop(columns=['Order_Date_only', 'Date_only'], inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "29c9bae8-1e3d-47a8-8af9-26b22ba1f512",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Order Date  Deaths_per_million\n",
      "0 2017-11-08                 NaN\n",
      "1 2017-11-08                 NaN\n",
      "2 2017-11-08                 NaN\n",
      "3 2017-11-08                 NaN\n",
      "4 2017-11-08                 NaN\n"
     ]
    }
   ],
   "source": [
    "print(df_final[['Order Date', 'Deaths_per_million']].head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afa242fc-c211-4184-ad4b-a5c5f8ca2ca6",
   "metadata": {},
   "source": [
    "##  Merge CPI Data with Main DataFrame\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "ef25f856-9b92-43fc-9d82-52aa4571a425",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Create a clean date-only column for merging\n",
    "df_final['Order_Date_only'] = df_final['Order Date'].dt.to_period('M').dt.to_timestamp()\n",
    "\n",
    "# Step 2: Round CPI Date to first of month (if needed, already should be in 'YYYY-MM-01')\n",
    "cpi_cleaned['CPI_Date'] = cpi_cleaned['Date'].dt.to_period('M').dt.to_timestamp()\n",
    "\n",
    "# Step 3: Merge on these normalized month-dates\n",
    "df_final = df_final.merge(\n",
    "    cpi_cleaned[['CPI_Date', 'CPI']],\n",
    "    left_on='Order_Date_only',\n",
    "    right_on='CPI_Date',\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# Step 4: Drop helper columns\n",
    "df_final.drop(columns=['Order_Date_only', 'CPI_Date'], inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "48b73039-b592-4460-839e-131c05336bbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Order Date      CPI\n",
      "0 2017-11-08  246.669\n",
      "1 2017-11-08  246.669\n",
      "2 2017-11-08  246.669\n",
      "3 2017-11-08  246.669\n",
      "4 2017-11-08  246.669\n"
     ]
    }
   ],
   "source": [
    "print(df_final[['Order Date', 'CPI']].head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82b39a0d-e9fd-4946-a4c1-e57554554ae6",
   "metadata": {},
   "source": [
    "##  9. Load and Clean Gas Price Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "91caed80-0469-4aeb-b4c2-e90a67950d4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Gas data cleaned. Shape: (1803, 2)\n",
      "        Date  Price\n",
      "0 2025-04-14  3.168\n",
      "1 2025-04-07  3.243\n",
      "2 2025-03-31  3.162\n",
      "3 2025-03-24  3.115\n",
      "4 2025-03-17  3.058\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pandas import PeriodIndex\n",
    "\n",
    "# Load dataset, skipping first 5 rows and setting column names\n",
    "gas_prices_df = pd.read_csv(\n",
    "    \"Weekly_U.S._Regular_All_Formulations_Retail_Gasoline_Prices.csv\",\n",
    "    skiprows=5,\n",
    "    names=[\"Date\", \"Price\"]\n",
    ")\n",
    "\n",
    "# Convert Date column to datetime\n",
    "gas_prices_df[\"Date\"] = pd.to_datetime(gas_prices_df[\"Date\"], errors='coerce')\n",
    "\n",
    "# Drop rows with missing Date or Price\n",
    "gas_prices_df.dropna(subset=[\"Date\", \"Price\"], inplace=True)\n",
    "\n",
    "# Convert Price to numeric\n",
    "gas_prices_df[\"Price\"] = pd.to_numeric(gas_prices_df[\"Price\"], errors=\"coerce\")\n",
    "\n",
    "# Drop rows with missing or invalid Price\n",
    "gas_prices_df.dropna(subset=[\"Price\"], inplace=True)\n",
    "\n",
    "print(\"✅ Gas data cleaned. Shape:\", gas_prices_df.shape)\n",
    "print(gas_prices_df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c374ccc6-120d-4b4d-8074-8503691e2855",
   "metadata": {},
   "source": [
    "##  Add Weekly Period Column to DataFrames\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "f0815e5e-f69e-4ccf-aaa5-f86435d1f745",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📅 Weekly column added to both DataFrames.\n"
     ]
    }
   ],
   "source": [
    "# Ensure 'Order Date' in df_final is datetime\n",
    "df_final[\"Order Date\"] = pd.to_datetime(df_final[\"Order Date\"], errors=\"coerce\")\n",
    "\n",
    "# Create weekly period columns\n",
    "df_final[\"Order_Week\"] = PeriodIndex(df_final[\"Order Date\"], freq=\"W\").start_time\n",
    "gas_prices_df[\"Order_Week\"] = PeriodIndex(gas_prices_df[\"Date\"], freq=\"W\").start_time\n",
    "\n",
    "print(\"📅 Weekly column added to both DataFrames.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f853a1c1-7ef3-4222-930e-6866b8aee653",
   "metadata": {},
   "source": [
    "##  Merge Gas Price Data into Main DataFrame\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "6adf0d09-cde9-4c53-b654-d21e950bc84e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Merged final shape: (1794015, 27)\n",
      "  Order Date Order_Week  Gasoline_Price\n",
      "0 2017-11-08 2017-11-06           2.561\n",
      "1 2017-11-08 2017-11-06           2.561\n",
      "2 2017-11-08 2017-11-06           2.561\n",
      "3 2017-11-08 2017-11-06           2.561\n",
      "4 2017-11-08 2017-11-06           2.561\n"
     ]
    }
   ],
   "source": [
    "# Merge gas price data into df_final based on 'Order_Week'\n",
    "df_final = pd.merge(df_final, gas_prices_df[[\"Order_Week\", \"Price\"]], on=\"Order_Week\", how=\"left\")\n",
    "\n",
    "# Optional: Rename for clarity\n",
    "df_final.rename(columns={\"Price\": \"Gasoline_Price\"}, inplace=True)\n",
    "\n",
    "print(\"✅ Merged final shape:\", df_final.shape)\n",
    "print(df_final[[\"Order Date\", \"Order_Week\", \"Gasoline_Price\"]].head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f3a6df0-7fd0-43e3-8171-af8dc44d3e71",
   "metadata": {},
   "source": [
    "##  Save Final Dataset to CSV\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "81cd72e3-f9a0-458e-b1be-7eddfbefd01a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Final dataset saved as 'retail_enriched_dataset.csv'\n"
     ]
    }
   ],
   "source": [
    "\n",
    "df_final.to_csv(\"retail_enriched_dataset.csv\", index=False)\n",
    "\n",
    "print(\"✅ Final dataset saved as 'retail_enriched_dataset.csv'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78c7de38-bf10-415f-932b-e8a7767b910f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
